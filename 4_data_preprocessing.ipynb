{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case folding\n",
    "\n",
    "def case_folding(str_data):\n",
    "    return str_data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kalimat.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Digits Removal\n",
    "\n",
    "def digits_removal(str_data):\n",
    "    return re.sub('[0-9]+', '', str_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub('[0-9]+', '', kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breaking Hyphens\n",
    "\n",
    "def breaking_hyphens(str_data):\n",
    "    return re.sub('[-]', ' ', str_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub('[-]', ' ', kalimat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Punctuation Mark Removal\n",
    "\n",
    "def punctuation_mark_removal(str_data):\n",
    "    str_data_t = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', str_data)\n",
    "    str_data_t = re.sub(r'\\n',' ',str_data_t)\n",
    "    str_data_t = re.sub(r'[?|$.!_:\")(+,*&%#@]','',str_data_t)\n",
    "    str_data_t = re.sub(' +', ' ', str_data_t)\n",
    "    str_data_t = str_data_t.strip()\n",
    "    return str_data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def punctuation_mark_removal(str_data):\n",
    "    \n",
    "#     str_data_t = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', kalimat)\n",
    "#     str_data_t = re.sub(r'\\n',' ',kalimat)\n",
    "#     str_data_t = re.sub(r'[?|$.!_:\")(+,*&%#@]','',kalimat)\n",
    "#     str_data_t = re.sub(' +', ' ', kalimat)\n",
    "\n",
    "#     str_data_t = str_data_t.strip()\n",
    "#     return str_data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenizing(str_data):\n",
    "    return (word_tokenize(str_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# from nltk.stem import SnowballStemmer\n",
    "# nltk.download()\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "id_stem = StemmerFactory().create_stemmer()\n",
    "# en_stem = SnowballStemmer('english')\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    treebank_tag = pos_tag([word])[0][1]\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize(word):\n",
    "    pos_tag = get_wordnet_pos(word)\n",
    "    return lemmatizer.lemmatize(word, pos=pos_tag)\n",
    "\n",
    "def stemming(list_str_data):\n",
    "    list_str_data_t = []\n",
    "    for s in list_str_data:\n",
    "        content_id = id_stem.stem(s)\n",
    "        # content_en = en_stem.stem(s)\n",
    "        content_en = lemmatize(s)\n",
    "        content = content_id if content_en == s else content_en\n",
    "        list_str_data_t.append(content)\n",
    "    return list_str_data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword Removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopword_id = stopwords.words('indonesian')\n",
    "stopword_en = stopwords.words('english')\n",
    "stopword_all = stopword_id + stopword_en\n",
    "def stopword_removal(list_str_data):\n",
    "    list_str_data_t = []\n",
    "    for s in list_str_data:\n",
    "        if s not in stopword_all:\n",
    "            list_str_data_t.append(s)\n",
    "    return list_str_data_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing all\n",
    "def preprocessing(str_data):\n",
    "    str_data_t = case_folding(str_data)\n",
    "    str_data_t = digits_removal(str_data_t)\n",
    "    str_data_t = breaking_hyphens(str_data_t)\n",
    "    str_data_t = punctuation_mark_removal(str_data_t)\n",
    "    str_data_t = tokenizing(str_data_t)\n",
    "    str_data_t = stemming(str_data_t)\n",
    "    str_data_t = stopword_removal(str_data_t)\n",
    "    return str_data_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing('menggali. memoto32ng digging going the me i tidak itu45 123. )( large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/data_5000best_preproc3/p03_5000best.csv',index_col=0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>url_file_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>https://www.betfair.com/sport/tennis/wta-melbo...</td>\n",
       "      <td>Bet on Tennis with Betfair™ Sportsbook and bro...</td>\n",
       "      <td>Tennis Betting &amp; Tennis Odds » Betfair™ Sports...</td>\n",
       "      <td>text/html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>https://www.betfair.com/sport/tennis/wta-melbo...</td>\n",
       "      <td>Bet on Tennis with Betfair™ Sportsbook and bro...</td>\n",
       "      <td>Tennis Betting &amp; Tennis Odds » Betfair™ Sports...</td>\n",
       "      <td>text/html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>https://www.betfair.com/sport/football/french-...</td>\n",
       "      <td>Bet on Football with Betfair™ Sportsbook and b...</td>\n",
       "      <td>Football Betting &amp; Football Odds Online » Betf...</td>\n",
       "      <td>text/html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>https://m.etsy.com/</td>\n",
       "      <td>Find the perfect handmade gift, vintage &amp; on-t...</td>\n",
       "      <td>Etsy - Shop for handmade, vintage, custom, and...</td>\n",
       "      <td>text/html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>https://www.betfair.com/sport/football/greek-s...</td>\n",
       "      <td>Bet on Football with Betfair™ Sportsbook and b...</td>\n",
       "      <td>Football Betting &amp; Football Odds Online » Betf...</td>\n",
       "      <td>text/html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "395   https://www.betfair.com/sport/tennis/wta-melbo...   \n",
       "386   https://www.betfair.com/sport/tennis/wta-melbo...   \n",
       "509   https://www.betfair.com/sport/football/french-...   \n",
       "1227                                https://m.etsy.com/   \n",
       "323   https://www.betfair.com/sport/football/greek-s...   \n",
       "\n",
       "                                            description  \\\n",
       "395   Bet on Tennis with Betfair™ Sportsbook and bro...   \n",
       "386   Bet on Tennis with Betfair™ Sportsbook and bro...   \n",
       "509   Bet on Football with Betfair™ Sportsbook and b...   \n",
       "1227  Find the perfect handmade gift, vintage & on-t...   \n",
       "323   Bet on Football with Betfair™ Sportsbook and b...   \n",
       "\n",
       "                                                  title url_file_type  \n",
       "395   Tennis Betting & Tennis Odds » Betfair™ Sports...     text/html  \n",
       "386   Tennis Betting & Tennis Odds » Betfair™ Sports...     text/html  \n",
       "509   Football Betting & Football Odds Online » Betf...     text/html  \n",
       "1227  Etsy - Shop for handmade, vintage, custom, and...     text/html  \n",
       "323   Football Betting & Football Odds Online » Betf...     text/html  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data\\data_preproc3\\p3_v2_log_combine_2022-01_b0_c00.csv',index_col=0)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description_p4'] = df.description.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_p4'] = df.title.apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/data_preproc4/p4_v2_log_combine_2022-01_b0_c00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data/data_5000best_preproc4/p04_v02_5000best.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['d_case_folding'] = df.description.apply(case_folding)\n",
    "df['d_digits_removal'] = df.d_case_folding.apply(digits_removal)\n",
    "df['d_breaking_hyphens'] = df.d_digits_removal.apply(breaking_hyphens)\n",
    "df['d_punctuation_mark_removal'] = df.d_breaking_hyphens.apply(punctuation_mark_removal)\n",
    "df['d_tokenizing'] = df.d_punctuation_mark_removal.apply(tokenizing)\n",
    "df['d_stemming'] = df.d_tokenizing.apply(stemming)\n",
    "df['d_stopword_removal'] = df.d_stemming.apply(stopword_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['t_case_folding'] = df.title.apply(case_folding)\n",
    "df['t_digits_removal'] = df.t_case_folding.apply(digits_removal)\n",
    "df['t_breaking_hyphens'] = df.t_digits_removal.apply(breaking_hyphens)\n",
    "df['t_punctuation_mark_removal'] = df.t_breaking_hyphens.apply(punctuation_mark_removal)\n",
    "df['t_tokenizing'] = df.t_punctuation_mark_removal.apply(tokenizing)\n",
    "df['t_stemming'] = df.t_tokenizing.apply(stemming)\n",
    "df['t_stopword_removal'] = df.t_stemming.apply(stopword_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/data_5000best_preproc4/p04_step_5000best.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "343f3d274de0554954cf955edb183fdc0cc62c5d26a777b2092a313e42e0680b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
